- **generate_embedding.py** 의 하이퍼 파라미터는 다음과 같습니다.
- 이를 참고해 쉘 스크립트를 수정해서 사용하면 됩니다. (파일 경로는 수정하지 않는 것이 좋습니다.)

|Hyper Parameter|설명|
|---|---|
|model|텍스트를 임베딩으로 변환하는 **Context Encoder** 의 경로입니다. 모델이 저장된 로컬 경로나 HuggingFace의 모델 명을 입력하면 됩니다.|
|wiki_path|**위키피디아 덤프**의 경로입니다. '../wikidump/text'가 기본값으로 설정되어 있습니다.|
|valid_data|평가에 사용하는 검증 **Validation Set** 의 경로입니다. 경로를 입력하지 않으면 Validation Set을 제외한 채 위키피디아 덤프만 임베딩으로 변환합니다. 위키피디아 덤프를 이용해 검색 성능을 평가하기 위해서는 반드시 경로를 입력해주어야 합니다.|
|save_path|Faiss Index와 BM25 모델이 담긴 **pickle 파일**이 저장되는 경로입니다. default='../pickles'가 기본값으로 설정되어 있습니다.|
|save_context|**제목과 텍스트**도 함께 pickle 파일(약 800MB)로 저장합니다. 단순히 검색 성능을 평가할 때는 저장할 필요가 없으나 직접 Semantic Search를 하기 위해서는 제목과 텍스트를 함께 저장해야 합니다. 쉘 스크립트에는 제목과 텍스트도 저장되도록 설정되어 있습니다.
|train_bm25|임베딩을 만들 때 사용했던 동일한 텍스트 집합으로 **BM25 모델**을 학습합니다. rank_bm25 라이브러리의 특성으로 인해 임베딩을 만들 때 사용했던 텍스트 집합과 동일한 집합으로 학습해야 BM25모델을 Reranking에 사용할 수 있습니다.|
|num_sent|위키피디아 덤프를 chunk로 분할 할 때 하나의 chunk에 포함된 **문장의 수**입니다. 5가 기본값으로 설정되어 있습니다.|
|overlap|연속된 chunk 간에 서로 **겹치는 문장의 수**입니다. 0이 기본값으로 설정되어 있습니다.|
|pooler|모델의 출력으로 부터 임베딩을 추출할 때 사용하는 **pooler**의 종류입니다. 'pooler_output', 'cls', 'mean', 'max' 중에 선택할 수 있고, 'cls'가 기본값으로 설정되어 있습니다.|
|max_length|Context Encoder 토크나이저의 **최대 토큰의 개수**입니다. BERT 모델의 최대 토큰 개수인 512 가 기본값으로 설정되어 있습니다.|
|batch_size|Context Encoder에 전달하는 데이터로 구성된 **배치의 크기**입니다. 32가 기본값으로 설정되어 있습니다.|
|cpu_workers|위키피디아 덤프를 chunk로 분할하는 과정은 Multi-processing 으로 이루어지는데, 이때 사용할 **cpu 코어의 개수**입니다. 쉘 스크립트에서 해당 부분을 제거하면 자동으로 모든 cpu 코어를 사용합니다. 많은 cpu 코어를 이용할 수록 위키피디아 덤프를 빠르게 분할할 수 있습니다.|
|device|'torch.cuda.is_available()'의 값에 따라 사용할 **장치(gpu/cpu)** 를 결정합니다. 여러 대의 gpu를 사용할 경우 쉘 스크립트에서 GPU가 명시된 부분을 수정해야 합니다.|
|random_seed|실행 결과를 고정하기 위한 **랜덤 시드**로, 42가 기본값으로 설정되어 있습니다.|
